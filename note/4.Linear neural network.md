# 线性回归模型


对模型中细节的思考和课后题
> linear-regression
> 
> 1. [3.1课后题](#课后题3_1linear_regression)
> 
> linear-regression-scratch
> 1. [yield预处理数据 ](#yield )
> 2. [课后题第五题在定义损失函数时为什么要reshape](#在定义损失函数时为什么要reshape)
> 3. [定义优化算法为什么要用到with_torch_no_grad](#定义优化算法为什么要用到with_torch_no_grad)
> 4. [3.2课后题](#课后题3_2linear-regression-scratch)
>    
> linear-regression-concise
> 1. [深度学习框架中获取数据--更加简易--将数据根据batch分好组](#数据迭代器)
> 2. [对模型参数手动更新为啥要有点data](#对模型参数手动更新为啥要有点data)
> 3. [3.3课后题](#课后题3_3linear_regression_concise)
>
> softmax-regression
> 1. [3.4课后题](#课后_3_4_softmax_regression)
>
> image-classification-dataset
> 1. [3.5课后题](#课后_3_5_image_classification_dataset)

## 以下部分内容是基础知识总结，想看精华，请通过上面链接跳转

### 基本表达式
> - $\widehat{y}$ =w<sub>1 </sub>x<sub>2</sub>+w<sub>2</sub>x<sub>2</sub>+.....+w<sub>n</sub>x<sub>n</sub>      又称输入特征的**仿射变换**
> - $\widehat{y}$ =w<sup>T</sup>x+b        特征向量x，权重w，**处理一个样本**
> - $\widehat{y}$ =X*w+b                   特征集合X（样本集合，一样一个样本） **一次性处理多个样本**

### 一种模型质量的度量方式
#### 损失函数
> 量化目标的实际值和预测值之间的差距
1. 平方误差

$$
\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.\]
$$

2. n个训练样本的损失均值

$$
\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]
$$

3. 最小参数下的样本损失

$$
\ [\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\ L(\mathbf{w}, b).\]
$$

$$
\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]  ~~~~~~~~~~~~(4.1)
$$

#### 带Batch的随机梯度下降
参数一次的更新过程（一次沿梯度反向下降） $~~~~~~$($\(\partial\)$表示偏导数)

$$
\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]
$$

> **正向传播大致过程**
> 1. 初始化模型参数的值，如随机初始化
> 2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:

$$
\[\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b) = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}\]
$$

$ \(\eta\)$表示学习率（learning rate）

#### 矢量化加速-->同时处理整个小批量样本

#### 解析解
> 能通过公式推导直接获得问题的解 eg：一元二次方程的两个解可以用a、b、c三个参数的表达式表示
有个矩阵求导举例 

#### 求参数w和b————————>>最小化均方误差等价于对线性模型的最大似然估计（高斯噪声假设下）
> 最大似然估计：已知结果，求过程中参数的最大可能值；用概率函数描述结果发生的可能性，当结果已知，说明该函数的值应为最大，因为已经发生，值越大，发生的概率越大
 1. 含噪声的目标函数

$$
\[y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,\] ~~~~~\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)
$$

 2. 给定x下y的似然函数

$$
\[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).\]
$$

 3. 根据极大似然估计法，参数w和b的最优值是**整个**数据集的似然函数的最大值

$$
\[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).\]
$$

4. 为方便求最大值，先去对数（方便计算）再求导

$$
\[-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.\]
$$

**现在我们只需要假设 $\(\sigma\)$ 是某个固定常数就可以忽略第一项， 因为第一项不依赖于 $\(\mathbf{w}\)$ 和 $\(b\)$ 。 现在第二项除了常数 $\(\frac{1}{\sigma^2}\)$ 外，其余部分和前面介绍的均方误差是一样的。**


### yield 
功能：与return一样返回当前值，之后的程序不再运行。同时生成**迭代器**，当下次再运行此段函数时，从yield此处断电开始运行而不是从头开始。有一个next()函数，表示执行yield迭代器的下一步操作。

目的：减少内存的使用

[yield的详细解释](https://blog.csdn.net/mieleizhi0522/article/details/82142856)

### 在定义损失函数时为什么要reshape
损失函数（均方损失）
>
``` py
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2  #返回返回(10，1)
```
答： 正常应该是同样的shape，但以防一个是行向量、一个是列向量。现在reshape，可以确保shape一样
> 技巧在做张量计算时，提前reshape，防止形状不同导致计算失败

### 定义优化算法为什么要用到with_torch_no_grad
答：在优化算法中，我们只需要根据已经计算好的梯度来更新模型参数，而不需要再次计算梯度。因此，在优化算法的代码块中，可以使用 torch.no_grad() 来关闭梯度的计算，如果下面有求梯度的代码，不会更新梯度，提高计算效率。

追问：已经计算好的梯度是在哪里计算的

答：梯度的计算通常是通过反向传播（Backpropagation）算法来实现的。当执行正向传播计算模型的输出和损失后，调用 .backward() 方法就可以自动计算梯度。这些梯度信息会被存储在张量的 .grad 属性中。


### 数据迭代器
``` py
from torch.utils import data
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)               #初始data_array是(features, labels)，首先通过*解包，将(features, labels)分开成两个张量
    return data.DataLoader(dataset, batch_size, shuffle=is_train)    #将两个张量同时按batch分组，每组包含相同数量的features和labels；is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。
batch_size = 10
data_iter = load_array((features, labels), batch_size)    #next(iter(data_iter))，使用iter构造Python迭代器，并使用next从迭代器中获取第一项。
```

``` py
import torch
from torch.utils.data import DataLoader, TensorDataset

# 创建输入数据和标签数据
inputs = torch.randn(100, 3, 32, 32)  # 100 个 3 通道的 32x32 图像
labels = torch.randint(0, 10, (100,))  # 随机生成 100 个标签（0-9）

# 将数据组合成 TensorDataset 对象
dataset = TensorDataset(inputs, labels)

# 创建 DataLoader，指定批量大小和是否打乱数据顺序
batch_size = 10
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 遍历 DataLoader，查看每个元素的形式
for batch in data_loader:
    inputs_batch, labels_batch = batch
    print("输入数据批次形状:", inputs_batch.shape)  # 输出 (batch_size, 3, 32, 32)
    print("标签批次形状:", labels_batch.shape)  # 输出 (batch_size,)
    break  # 仅查看第一个批次
```

###         l.sum().backward()

### 对模型参数手动更新为啥要有点data
在神经网络中，模型的参数通常是张量对象，例如权重参数、偏置参数等。这些参数在进行正向传播和反向传播过程中会自动更新。然而，有时你可能希望对参数进行一些特定的操作（比如初始化），**而不想影响自动求导的过程**。在这种情况下，使用 .data 可以只获取张量的数据部分，**而不包括梯度信息**，从而避免对梯度的影响。

``` py
import torch

# 创建一个张量并设置 requires_grad=True，表示需要进行梯度计算
tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 直接对张量进行操作，可能会影响张量的值和梯度
# 这里是一个错误的示例，仅用于说明问题
# 在实际操作中应该使用 .data 或 with torch.no_grad() 来避免影响梯度计算
tensor = tensor * 2  # 这个操作会同时修改张量的值和梯度

# 进行梯度计算
result = tensor.sum()
result.backward()

# 输出梯度信息
print(tensor.grad)  # 输出梯度信息，结果可能不正确
```
在上面的示例中，我们直接对张量 tensor 进行操作（tensor = tensor * 2），这个操作不仅会修改张量的值，也会同时修改梯度信息。这样可能导致后续梯度计算的结果不正确，因为梯度信息已经被操作修改过了。梯度会*2


### 课后题(3_1linear_regression)

#### 1. 假设我们有一些数据 $x_1, \ldots, x_n \in \mathbb{R}$ 。我们的目标是找到一个常数 $\sum_i (x_i - b)^2$，使得最小化 。
1. 找到最优值b的解析解。

要找到使目标函数 $\sum_{i} (x_i - b)^2$ 最小化的常数 $b$ 的解析解，我们可以对目标函数关于 $b$ 求导并令导数等于零，然后解方程找到临界点。

目标函数为：

$$
\[
f(b) = \sum_{i} (x_i - b)^2
\]
$$

对 $b$ 求导：

$$
\[
f'(b) = \frac{d}{db} \sum_{i} (x_i - b)^2 = -2 \sum_{i} (x_i - b)
\]
$$

令导数等于零：

$$
\[
-2 \sum_{i} (x_i - b) = 0
\]
$$

解方程得：

$$
\[
\sum_{i} x_i - nb = 0
\]
$$

从而：

$$
\[
b = \frac{1}{n} \sum_{i} x_i
\]
$$

因此，解析解为数据 $x_1, \ldots, x_n$ 的均值。

2. 这个问题及其解与正态分布有什么关系?

我感觉取决与xi的分布情况，若xi满足正态分布，则b的解析解就是标准正态分布

#### 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置b（我们可以通过向1添加所有值为1的一列来做到这一点）。

##### 1. 用矩阵和向量表示法写出优化问题（将所有数据视为单个矩阵，将所有目标值视为单个向量）。

线性回归的优化问题可以用平方误差函数来表示。假设我们有 $m$ 个样本数据点，每个数据点包含 $n$ 个特征，我们可以将所有数据表示为一个 $m \times n$ 的矩阵 $X$，每行代表一个数据样本，每列代表一个特征。假设我们的目标是预测一个实值目标 $y$，我们可以将所有目标值表示为一个长度为 $m$ 的向量 $y$。参数向量 $\theta$ 则表示线性模型的系数。

线性回归的模型可以表示为：

$$
\[ \hat{y} = X \theta \]
$$

其中，$\hat{y}$ 是模型预测的目标值。

我们使用平方误差函数来定义损失函数 $J(\theta)$：

$$
\[ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 \]
$$

其中，$\hat{y}^{(i)}$  是模型对第 $i$ 个样本的预测值，$y^{(i)}$  是第 $i$ 个样本的真实目标值。

我们的目标是最小化损失函数 $J(\theta)$，即找到最优的参数 $\theta$。

使用矩阵和向量表示法，可以将损失函数 $J(\theta)$ 写成如下形式：

$$
\[ J(\theta) = \frac{1}{2m} (X\theta - y)^T (X\theta - y) \]
$$

我们的优化目标是最小化损失函数 $J(\theta)$ 关于参数 $\theta$ 的值。因此，我们需要解决以下优化问题：

$$
\[ \min_{\theta} J(\theta) = \min_{\theta} \frac{1}{2m} (X\theta - y)^T (X\theta - y) \]
$$

为了找到最优的参数 $\theta$，我们需要对损失函数 $J(\theta)$ 关于参数 $\theta$ 求导，并令导数等于零，然后解方程找到临界点。这将得到解析解 $\theta$ 的表达式。
> 具体求导过程见第一章[等待链接](#)

##### 2. 计算损失对w的梯度。

我们已经定义了损失函数 $J(\theta)$ 为：

$$
\[ J(\theta) = \frac{1}{2m} (X\theta - y)^T (X\theta - y) \]
$$

我们的目标是计算损失函数 $J(\theta)$ 关于参数 $\theta$ 的梯度。首先，我们展开损失函数，然后对 $\theta$ 求偏导数。

$$
\[
\begin{align*}
J(\theta) &= \frac{1}{2m} (X\theta - y)^T (X\theta - y) \\
&= \frac{1}{2m} (\theta^T X^T X \theta - 2 \theta^T X^T y + y^T y)
\end{align*}
\]
$$

现在我们对 $J(\theta)$ 关于 $\theta$ 求偏导数。首先我们计算每一项的偏导数：

1. 对于 $\theta^T X^T X \theta$，我们有：

$$
\[
\frac{\partial}{\partial \theta} (\theta^T X^T X \theta) = X^T X \theta + X^T X \theta = 2 X^T X \theta
\]
$$

2. 对于 $2 \theta^T X^T y$，我们有：

$$
\[
\frac{\partial}{\partial \theta} (2 \theta^T X^T y) = 2 X^T y
\]
$$

3. 对于 $y^T y$，由于 $\theta$ 不在 $y$ 中出现，其偏导数为零。

现在，我们将这些结果组合在一起，并将其除以 $m$ 得到最终的梯度表达式：

$$
\[
\nabla_{\theta} J(\theta) = \frac{1}{m} (X^T X \theta - X^T y)
\]
$$

这就是损失函数 $J(\theta)$ 关于参数 $\theta$ 的梯度表达式。

##### 3. 通过将梯度设为0、求解矩阵方程来找到解析解。

要通过将梯度设为零来求解线性回归的解析解，我们可以将损失函数的梯度设置为零，然后解出参数向量 $w$。首先，我们有损失函数的梯度为：

$$
\[ \nabla_{w} J(w) = \frac{1}{m} (X^T X w - X^T y) \]
$$

将梯度设为零：

$$
\[ \frac{1}{m} (X^T X w - X^T y) = 0 \]
$$

移项得：

$$
\[ X^T X w = X^T y \]
$$

现在，我们解矩阵方程 $X^T X w = X^T y$ 来找到参数向量 $w$ 的解析解。假设 $X^T X$ 是可逆的，那么我们可以左乘矩阵 $(X^T X)^{-1}$，得到：

$$
\[ w = (X^T X)^{-1} X^T y \]
$$

这就是线性回归问题的解析解，也称为最小二乘解。这个公式告诉我们，通过将数据矩阵 $X$ 和目标向量 $y$ 与它们的转置矩阵相乘，并乘以 $(X^T X)^{-1}$，我们可以直接计算出最优参数向量 $w$，而无需进行迭代优化算法。

##### 4. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？

随机梯度下降（SGD）和解析解（最小二乘法）两者在不同情况下可能表现更好或更差。以下是它们的优缺点以及适用情况：

**解析解（最小二乘法）优点：**

1. **全局最优解：** 解析解给出了损失函数的全局最优解，因此对于凸优化问题，能够快速找到最优解。
2. **计算简单：** 对于小规模的数据集和特征数量不太多的情况，计算解析解是非常快速的。

**解析解（最小二乘法）缺点：**

1. **计算复杂度高：** 对于大规模的数据集和高维特征空间，计算矩阵的逆 $(X^T X)^{-1}$ 的复杂度较高。
2. **内存消耗大：** 在数据量大的情况下，需要存储整个数据矩阵 $X$，内存消耗较大。

**随机梯度下降（SGD）优点：**

1. **适用于大规模数据：** 对于大规模的数据集和高维特征空间，SGD 的计算量比解析解小得多，适合于大规模的数据集和高维度的特征。
2. **内存消耗小：** SGD 每次只需要计算一个样本或一个小批量样本的梯度，内存消耗相对较小。

**随机梯度下降（SGD）缺点：**

1. **不稳定性：** SGD 的收敛速度不稳定，有时可能会陷入局部最优解，需要调节学习率等超参数来优化收敛性。
2. **噪声影响：** SGD 每次只考虑一个样本或小批量样本的梯度，因此可能会受到噪声的影响，导致收敛路径不稳定。

**何时使用解析解更好：**

- 当数据集规模较小，特征数量较少，并且计算资源充足时，解析解更好，因为它可以给出全局最优解，且计算速度较快。

**何时使用随机梯度下降更好：**

- 当数据集规模较大，特征数量较多，并且计算资源有限时，SGD 更好，因为它计算量小且内存消耗小，适合于大规模数据的情况。

**失效情况：**

- 当损失函数具有非凸性质或者存在大量噪声的情况下，SGD 可能会失效，因为它可能陷入局部最优解或者收敛速度很慢。此时可能需要使用改进的随机梯度下降算法，如带动量的随机梯度下降（Momentum SGD）、自适应学习率的随机梯度下降（Adagrad、RMSprop、Adam 等）。

#### 假定控制附加噪声 $\epsilon$ 的噪声模型是指数分布。也就是说， $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$ 

##### 1. 写出模型 $-\log P(\mathbf y \mid \mathbf X)$下数据的负对数似然。

首先，假设我们有一个噪声模型 $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$，其中 $\epsilon$ 是噪声。接着我们有一个线性回归模型 $y = Xw + \epsilon$，其中 $y$ 是目标变量，$X$ 是数据矩阵，$w$ 是模型参数，$\epsilon$ 是噪声。

我们知道，对数似然函数是指给定参数下观测数据出现的概率的对数。对于线性回归模型，我们可以假设观测数据的条件概率服从正态分布。但在这种情况下，噪声模型是指数分布，我们可以将数据的负对数似然写成：

$$
\[ -\log P(\mathbf{y} \mid \mathbf{X}) = -\log \prod_{i=1}^{n} p(y_i - \mathbf{x}_i^T w) \]
$$

$$
\[ = -\log \prod_{i=1}^{n} \frac{1}{2} \exp(-|y_i - \mathbf{x}_i^T w|) \]
$$

$$
\[ = -\sum_{i=1}^{n} \log \left(\frac{1}{2} \exp(-|y_i - \mathbf{x}_i^T w|)\right) \]
$$

$$
\[ = \sum_{i=1}^{n} \left(|y_i - \mathbf{x}_i^T w| - \log 2\right) \]
$$

$$
\[ = \sum_{i=1}^{n} |y_i - \mathbf{x}_i^T w| - n \log 2 \]
$$

这就是模型 $-\log P(\mathbf{y} \mid \mathbf{X})$ 下数据的负对数似然。注意到最后一项 $- n \log 2$ 是一个常数，对最小化负对数似然没有影响，因此我们可以简化为：

$$
\[ -\log P(\mathbf{y} \mid \mathbf{X}) = \sum_{i=1}^{n} |y_i - \mathbf{x}_i^T w| \]           
$$
> xi是(n,1),w是(n,1)
##### 2. 请试着写出解析解。
> 存在绝对值，所以不可导，故没有解析式
>
> 如果**解析解**没有，可以尝试**梯度下降法**。
> 
对于模型 $-\log P(\mathbf{y} \mid \mathbf{X}) = \sum_{i=1}^{n} |y_i - \mathbf{x}_i^T w|$ 的负对数似然，我们可以尝试求解其解析解。这个问题实际上是一个绝对值损失函数的最小化问题，也称为 L1 损失函数。最小化 L1 损失函数的解析解可能是一个复杂的问题，因为它不像平方损失函数那样具有解析解。

然而，我们可以尝试使用梯度下降等优化方法来近似求解最小化 L1 损失函数的参数 $w$。梯度下降的基本思想是沿着损失函数梯度的反方向更新参数 $w$，直到达到最小值或收敛。

梯度下降的更新规则为：

$$
\[ w := w - \alpha \nabla_{w} (-\log P(\mathbf{y} \mid \mathbf{X})) \]
$$

其中，$\alpha$ 是学习率，控制更新步长。

我们可以计算 L1 损失函数的梯度 $\nabla_{w} (-\log P(\mathbf{y} \mid \mathbf{X}))$，然后通过梯度下降算法逐步更新参数 $w$。

因此，对于给定的数据集 $\mathbf{X}$ 和目标变量 $\mathbf{y}$，我们可以使用梯度下降来近似求解 L1 损失函数的最小化问题。然而需要注意的是，由于 L1 损失函数具有非连续性，在优化过程中可能会遇到一些挑战，例如在梯度为零的点处需要特殊处理。

##### 3. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。
梯度下降达不到驻点（最优解），只能无限接近；同时，如果学习率较大，可能会在驻点附近来回跳跃

### 课后题(3_2linear-regression-scratch)

#### 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？
> 话不多说，直接上图

picture1：正常初始化权重

![初始化权重后的结果](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2b971a4ec12fe9bd6d2af7822f1d093d819df367/picture/3.2T1_%E6%AD%A3%E5%B8%B8%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%E7%9A%84%E7%BB%93%E6%9E%9C.png)

picture2：权重初始化为零

![初始化权重为0的结果](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/4643de7127fd0d5cc4c2f3c325fb334bc403d7b6/picture/3.2T1_%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%E4%B8%BA0%E7%9A%84%E7%BB%93%E6%9E%9C.png)

#### 假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗?
我感觉可以，V=IR,属于线性关系，存在微分
> GPT也说可以，还提供了代码
``` py
import torch
import torch.nn as nn
import torch.optim as optim

# 定义电压和电流关系模型
class VoltageCurrentModel(nn.Module):
    def __init__(self):
        super(VoltageCurrentModel, self).__init__()
        self.linear = nn.Linear(1, 1)  # 线性层，输入特征维度为1，输出特征维度为1

    def forward(self, voltage):
        current = self.linear(voltage)
        return current

# 创建模型实例
model = VoltageCurrentModel()

# 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差损失函数
optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD优化器

# 生成模拟数据
voltage = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
current_true = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# 训练模型
num_epochs = 1000
for epoch in range(num_epochs):
    # 正向传播
    current_pred = model(voltage)
    loss = criterion(current_pred, current_true)
    
    # 反向传播
    optimizer.zero_grad()  # 梯度清零
    loss.backward()  # 计算梯度
    optimizer.step()  # 更新参数
    
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 测试模型
test_voltage = torch.tensor([[5.0], [6.0]])
predicted_current = model(test_voltage)
print(f'Predicted Current for Test Voltage: {predicted_current.detach().numpy()}')
``` 

#### 能基于普朗克定律使用光谱能量密度来确定物体的温度吗？
> 超领域了吧,不会

#### 计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？
> GPT答案

1. 数值不稳定性： 在计算过程中可能会出现数值不稳定的情况，例如除以一个接近零的数，或者由于舍入误差导致计算结果不准确。
2. 计算复杂度高： 计算二阶导数通常需要进行两次微分，这可能会增加计算的复杂度和时间成本。
3. 梯度消失或爆炸： 在深度学习中，当计算二阶导数时，有时会出现梯度消失或者梯度爆炸的问题，特别是在深层神经网络中。

这些问题可以通过一些方法来解决：

1. 数值稳定性处理： 在计算过程中，可以采用数值稳定性处理的方法，例如使用数值稳定的数值计算库、避免除以接近零的数、采用高精度数值计算等。
2. 符号计算： 对于简单的函数，可以使用符号计算的方法来精确计算二阶导数，而不是通过数值近似。
3. 使用自动微分库： 使用深度学习框架或者自动微分库进行计算时，这些库通常会处理数值稳定性和梯度消失等问题，可以更方便地计算二阶导数。
4.正则化和归一化： 在深度学习中，可以通过正则化方法（如权重衰减）和归一化方法（如批量归一化）来减少梯度消失或者爆炸的问题。

#### 为什么在squared_loss函数中需要使用reshape函数？
详情见[本题答案](#在定义损失函数时为什么要reshape)

#### 尝试使用不同的学习率，观察损失函数值下降的快慢。

**注意每改一次学习率，w和b等参数要重置**
1. lr=0.03

epoch 1, loss 0.058343
epoch 2, loss 0.000294
epoch 3, loss 0.000053

2. lr=0.3

epoch 1, loss 0.000054
epoch 2, loss 0.000054
epoch 3, loss 0.000056

3. lr=0.01

epoch 1, loss 2.408839
epoch 2, loss 0.386380
epoch 3, loss 0.062431

> 结论：学习率越大，损失函数下降越快

#### 如果样本个数不能被批量大小整除，data_iter函数的行为会有什么变化？
最后一组的样本个数<batch_size。
``` py
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)#用于将一个列表中的元素打乱顺序
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])    $~~~~~~~~~~~~~~~~~~~~~~~~$    #min函数在保证不超边界的情况下，让最后一组的样本数量可以<batch_size
        yield features[batch_indices], labels[batch_indices]
```


### 课后题(3_3linear_regression_concise)

#### 如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？
> 经评论区反应题目有问题，不过不影响本质，他想考察**损失函数与学习率**，根据公式，先求小批量的梯度在除以batch_size。

所以，从nn.MSELoss（）（默认是mean）改为nn.MSELoss(reduction=‘sum‘），学习率要除以batch_size，因为求和后梯度*batch_size，扩大了。

**经网友实验验证得出：明显可见地选用sum而不是mean将很大程度影响模型的学习效果，mean更好**

#### 查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？

> 损失函数文档
``` py
import torch
import webbrowser

# 损失函数文档链接
loss_functions_url = 'https://pytorch.org/docs/stable/nn.html#loss-functions'

# 使用 webbrowser 打开链接
webbrowser.open(loss_functions_url)
```

> 损失函数文档
``` py
import torch
import webbrowser


# 初始化方法文档链接
initializers_url = 'https://pytorch.org/docs/stable/nn.init.html'

# 使用 webbrowser 打开链接
webbrowser.open(initializers_url)

``` 

#### 如何访问线性回归的梯度？
``` py
#net = nn.Sequential(nn.Linear(2, 1)) 只有一个线性回归层的深度学习框架
print(net[0].weight.grad)
```

### 课后_3_4_softmax_regression

#### 1.我们可以更深入地探讨指数族与softmax之间的联系。

##### 1.1计算softmax交叉熵损失 $l(\mathbf{y},\hat{\mathbf{y}})$ 的二阶导数。

### 课后_3_5_image_classification_dataset

#### 1. 减少batch_size（如减少到1）是否会影响读取性能？
``` PY
batch_size = 512      TIME=5.23 sec
batch_size = 256      TIME=5.20 sec
batch_size = 128      TIME=5.14 sec
batch_size = 64       TIME=5.12 sec
batch_size = 32       TIME=5.13 sec   #第二次跑5.26 sec
batch_size = 1        TIME=16.19 sec
```
> **batch_size=256时效果最佳，是最小点**

#### 2. 数据迭代器的性能非常重要。当前的实现足够快吗？探索各种选择来改进它。
PASS
#### 3. 查阅框架的在线API文档。还有哪些其他数据集可用？
ImageNet： 包含超过1400万张带有标签的图像，涵盖了超过2万个类别。

CIFAR-10 和 CIFAR-100： 分别包含60000张32x32彩色图像，涵盖了10个类别和100个类别。

MNIST： 包含手写数字图像的数据集，共有60000个训练样本和10000个测试样本。

COCO（Common Objects in Context）： 包含超过330000张图像，涵盖了80个常见物体类别，每个图像都有多个物体实例的标注信息。

PASCAL VOC（Visual Object Classes）： 包含20个物体类别的图像数据集，每个图像都有多个物体实例的标注信息。

UCI机器学习库： 包含了大量常用的机器学习数据集，涵盖了各种类型的数据。
