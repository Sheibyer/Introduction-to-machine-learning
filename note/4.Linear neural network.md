# 线性回归模型


对模型中细节的思考
> linear-regression-scratch
> 1. [yield预处理数据 ](#yield )
> 2. [课后题第五题在定义损失函数时为什么要reshape](#在定义损失函数时为什么要reshape)
> 3. [定义优化算法为什么要用到with_torch_no_grad](#定义优化算法为什么要用到with_torch_no_grad)
> 4. [3.2课后题](#课后题3_2linear-regression-scratch)
>    
> linear-regression-concise
> 1. [深度学习框架中获取数据--更加简易--将数据根据batch分好组](#数据迭代器)
> 2. [对模型参数手动更新为啥要有点data](#对模型参数手动更新为啥要有点data)
> 3. [3.3课后题](#课后题3_3linear_regression_concise)
### 基本表达式
> - $\widehat{y}$ =w<sub>1 </sub>x<sub>2</sub>+w<sub>2</sub>x<sub>2</sub>+.....+w<sub>n</sub>x<sub>n</sub>      又称输入特征的**仿射变换**
> - $\widehat{y}$ =w<sup>T</sup>x+b        特征向量x，权重w，**处理一个样本**
> - $\widehat{y}$ =X*w+b                   特征集合X（样本集合，一样一个样本） **一次性处理多个样本**

### 一种模型质量的度量方式
#### 损失函数
> 量化目标的实际值和预测值之间的差距
1. 平方误差

$$
\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.\]
$$

2. n个训练样本的损失均值

$$
\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]
$$

3. 最小参数下的样本损失

$$
\ [\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\ L(\mathbf{w}, b).\]
$$

$$
\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]  ~~~~~~~~~~~~(4.1)
$$

#### 带Batch的随机梯度下降
参数一次的更新过程（一次沿梯度反向下降） $~~~~~~$($\(\partial\)$表示偏导数)

$$
\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]
$$

> **正向传播大致过程**
> 1. 初始化模型参数的值，如随机初始化
> 2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:

$$
\[\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b) = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}\]
$$

$ \(\eta\)$表示学习率（learning rate）

#### 矢量化加速-->同时处理整个小批量样本

#### 解析解
> 能通过公式推导直接获得问题的解 eg：一元二次方程的两个解可以用a、b、c三个参数的表达式表示
有个矩阵求导举例 

#### 求参数w和b————————>>最小化均方误差等价于对线性模型的最大似然估计（高斯噪声假设下）
> 最大似然估计：已知结果，求过程中参数的最大可能值；用概率函数描述结果发生的可能性，当结果已知，说明该函数的值应为最大，因为已经发生，值越大，发生的概率越大
 1. 含噪声的目标函数

$$
\[y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,\] ~~~~~\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)
$$

 2. 给定x下y的似然函数

$$
\[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).\]
$$

 3. 根据极大似然估计法，参数w和b的最优值是**整个**数据集的似然函数的最大值

$$
\[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).\]
$$

4. 为方便求最大值，先去对数（方便计算）再求导

$$
\[-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.\]
$$

**现在我们只需要假设 $\(\sigma\)$ 是某个固定常数就可以忽略第一项， 因为第一项不依赖于 $\(\mathbf{w}\)$ 和 $\(b\)$ 。 现在第二项除了常数 $\(\frac{1}{\sigma^2}\)$ 外，其余部分和前面介绍的均方误差是一样的。**


### yield 
功能：与return一样返回当前值，之后的程序不再运行。同时生成**迭代器**，当下次再运行此段函数时，从yield此处断电开始运行而不是从头开始。有一个next()函数，表示执行yield迭代器的下一步操作。

目的：减少内存的使用

[yield的详细解释](https://blog.csdn.net/mieleizhi0522/article/details/82142856)

### 在定义损失函数时为什么要reshape
损失函数（均方损失）
>
``` py
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2  #返回返回(10，1)
```
答： 正常应该是同样的shape，但以防一个是行向量、一个是列向量。现在reshape，可以确保shape一样
> 技巧在做张量计算时，提前reshape，防止形状不同导致计算失败

### 定义优化算法为什么要用到with_torch_no_grad
答：在优化算法中，我们只需要根据已经计算好的梯度来更新模型参数，而不需要再次计算梯度。因此，在优化算法的代码块中，可以使用 torch.no_grad() 来关闭梯度的计算，如果下面有求梯度的代码，不会更新梯度，提高计算效率。

追问：已经计算好的梯度是在哪里计算的

答：梯度的计算通常是通过反向传播（Backpropagation）算法来实现的。当执行正向传播计算模型的输出和损失后，调用 .backward() 方法就可以自动计算梯度。这些梯度信息会被存储在张量的 .grad 属性中。


### 数据迭代器
``` py
from torch.utils import data
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)               #初始data_array是(features, labels)，首先通过*解包，将(features, labels)分开成两个张量
    return data.DataLoader(dataset, batch_size, shuffle=is_train)    #将两个张量同时按batch分组，每组包含相同数量的features和labels；is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。
batch_size = 10
data_iter = load_array((features, labels), batch_size)    #next(iter(data_iter))，使用iter构造Python迭代器，并使用next从迭代器中获取第一项。
```

``` py
import torch
from torch.utils.data import DataLoader, TensorDataset

# 创建输入数据和标签数据
inputs = torch.randn(100, 3, 32, 32)  # 100 个 3 通道的 32x32 图像
labels = torch.randint(0, 10, (100,))  # 随机生成 100 个标签（0-9）

# 将数据组合成 TensorDataset 对象
dataset = TensorDataset(inputs, labels)

# 创建 DataLoader，指定批量大小和是否打乱数据顺序
batch_size = 10
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 遍历 DataLoader，查看每个元素的形式
for batch in data_loader:
    inputs_batch, labels_batch = batch
    print("输入数据批次形状:", inputs_batch.shape)  # 输出 (batch_size, 3, 32, 32)
    print("标签批次形状:", labels_batch.shape)  # 输出 (batch_size,)
    break  # 仅查看第一个批次
```

###         l.sum().backward()

### 对模型参数手动更新为啥要有点data
在神经网络中，模型的参数通常是张量对象，例如权重参数、偏置参数等。这些参数在进行正向传播和反向传播过程中会自动更新。然而，有时你可能希望对参数进行一些特定的操作（比如初始化），**而不想影响自动求导的过程**。在这种情况下，使用 .data 可以只获取张量的数据部分，**而不包括梯度信息**，从而避免对梯度的影响。

``` py
import torch

# 创建一个张量并设置 requires_grad=True，表示需要进行梯度计算
tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 直接对张量进行操作，可能会影响张量的值和梯度
# 这里是一个错误的示例，仅用于说明问题
# 在实际操作中应该使用 .data 或 with torch.no_grad() 来避免影响梯度计算
tensor = tensor * 2  # 这个操作会同时修改张量的值和梯度

# 进行梯度计算
result = tensor.sum()
result.backward()

# 输出梯度信息
print(tensor.grad)  # 输出梯度信息，结果可能不正确
```
在上面的示例中，我们直接对张量 tensor 进行操作（tensor = tensor * 2），这个操作不仅会修改张量的值，也会同时修改梯度信息。这样可能导致后续梯度计算的结果不正确，因为梯度信息已经被操作修改过了。梯度会*2
### 课后题(3_2linear-regression-scratch)

#### 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？
> 话不多说，直接上图

picture1：正常初始化权重

![初始化权重后的结果]()

picture2：权重初始化为零

![初始化权重为0的结果]()










### 课后题(3_3linear_regression_concise)

#### 如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？
> 经评论区反应题目有问题，不过不影响本质，他想考察**损失函数与学习率**，根据公式，先求小批量的梯度在除以batch_size。

所以，从nn.MSELoss（）（默认是mean）改为nn.MSELoss(reduction=‘sum‘），学习率要除以batch_size，因为求和后梯度*batch_size，扩大了。

**经网友实验验证得出：明显可见地选用sum而不是mean将很大程度影响模型的学习效果，mean更好**

#### 查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？

> 损失函数文档
``` py
import torch
import webbrowser

# 损失函数文档链接
loss_functions_url = 'https://pytorch.org/docs/stable/nn.html#loss-functions'

# 使用 webbrowser 打开链接
webbrowser.open(loss_functions_url)
```

> 损失函数文档
``` py
import torch
import webbrowser


# 初始化方法文档链接
initializers_url = 'https://pytorch.org/docs/stable/nn.init.html'

# 使用 webbrowser 打开链接
webbrowser.open(initializers_url)

``` 

#### 如何访问线性回归的梯度？
``` py
#net = nn.Sequential(nn.Linear(2, 1)) 只有一个线性回归层的深度学习框架
print(net[0].weight.grad)
```



