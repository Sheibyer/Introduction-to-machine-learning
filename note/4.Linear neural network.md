# 线性回归模型
### 基本表达式
> - $\widehat{y}$ =w<sub>1 </sub>x<sub>2</sub>+w<sub>2</sub>x<sub>2</sub>+.....+w<sub>n</sub>x<sub>n</sub>      又称输入特征的**仿射变换**
> - $\widehat{y}$ =w<sup>T</sup>x+b        特征向量x，权重w，**处理一个样本**
> - $\widehat{y}$ =X*w+b                   特征集合X（样本集合，一样一个样本） **一次性处理多个样本**

### 一种模型质量的度量方式
#### 损失函数
> 量化目标的实际值和预测值之间的差距
1. 平方误差

$$
\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.\]
$$

2. n个训练样本的损失均值

$$
\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]
$$

3. 最小参数下的样本损失

$$
\ [\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\ L(\mathbf{w}, b).\]
$$

$$
\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]  ~~~~~~~~~~~~(4.1)
$$

#### 带Batch的随机梯度下降
参数一次的更新过程（一次沿梯度反向下降） $~~~~~~$($\(\partial\)$表示偏导数)

$$
\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]
$$

> **正向传播大致过程**
> 1. 初始化模型参数的值，如随机初始化
> 2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:

$$
\[\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b) = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}\]
$$

$ \(\eta\)$表示学习率（learning rate）

#### 矢量化加速-->同时处理整个小批量样本

#### 解析解
> 能通过公式推导直接获得问题的解 eg：一元二次方程的两个解可以用a、b、c三个参数的表达式表示

#### 求参数w和b————————>>最小化均方误差等价于对线性模型的最大似然估计（高斯噪声假设下）
> 最大似然估计：已知结果，求过程中参数的最大可能值；用概率函数描述结果发生的可能性，当结果已知，说明该函数的值应为最大，因为已经发生，值越大，发生的概率越大
 1. 含噪声的目标函数

$$
\[y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,\] ~~~~~\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)
$$

 2. 给定x下y的似然函数

$$
\[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).\]
$$

 3. 根据极大似然估计法，参数w和b的最优值是**整个**数据集的似然函数的最大值

$$
\[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).\]
$$

4. 为方便求最大值，先去对数（方便计算）再求导

$$
\[-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.\]
$$

**现在我们只需要假设 $\(\sigma\)$ 是某个固定常数就可以忽略第一项， 因为第一项不依赖于 $\(\mathbf{w}\)$ 和 $\(b\)$ 。 现在第二项除了常数 $\(\frac{1}{\sigma^2}\)$ 外，其余部分和前面介绍的均方误差是一样的。**


