
[2课后题](#课后题)

### 标量
定义：只有一个元素的张量
``` py
x=tf.constant(1.0)
```

### 向量
用一维张量表示向量，长度任意.**列向量**是向量的默认方向
``` py
x=tf.range(4)
x[3]    #取向量的任意元素，是标量
```

### 长度、维度、形状
求张量长度
``` py
len(x)
x.shape
```

### 矩阵
- 求转置
``` py
tf.transpose(A)
```
- 对称矩阵
- Hadamard积：相同形状的张量对应元素相乘，数学符号为（一个圈里面一个点）
``` py
A*B
```

### 张量
描述具有任意数量轴的n维数组

### 降维
- 求和降维，沿某一条轴求和，结果中该轴消失
``` py
tf.reduce_sum(A,axis=0)    #按列求和，结果为1*n
tf.reduce_sum(A,axis=1)    #按行求和，结果为n*1
tf.reduce_sum(A,axis=[0,1])    #按行和列求和，结果为1*1，等同于tf.reduce_sum(A)
```
- 平均值
``` py
tf.reduce_mean(A)
tf.reduce_sum(A) / tf.size(A).numpy()
```
### 非降维求和
``` py
x=tf.reduce_sum(A,axis=1,keepdims=True) #结果保持原来的维度
```

### 点积
相同位置的按元素乘积的和
类似于先x*y，在reduce_sum
最总结果是一个元素
``` py
y = tf.ones(4, dtype=tf.float32)
x, y, tf.tensordot(x, y, axes=1)    #等同于tf.reduce_sum(x*y)
```
个人理解：假如x，y是4*1，x和y的点积是先x转置，变为1*4，然后x和y按照矩阵乘法相乘，结果为1*1。

### 矩阵-向量积
``` py
tf.linalg.matvec(A,x)    #shape:A:[5,4],x:[4] res[5,]
```

### 矩阵-矩阵乘法
``` py
tf.matmul(A,B)    #按照线性代数矩阵相乘的规则计算
```

### 范数
将向量映射到标量的函数
example：
- L2范数：元素平方和的平凡根

![图片](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/16c604aa2fbe775f62544e721dda1f4e29196d33/picture/L2%E8%8C%83%E5%BC%8F.PNG "L2范式")

通常L2范式的下标不写，||x||==||x||2
``` py
tf.norm(u)
```
- L1范数：元素按绝对值求和

![图片](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/fc7d98b6dca86f94f244c6e4cc1977f26a849d9c/picture/L1%E8%8C%83%E5%BC%8F.PNG)
``` py
tf.reduce_sum(tf.abs(u))
```
- Lp范式

![图片](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/7ed9d4ae4f854a3a085580f68c5515e9f95ac906/picture/LP%E8%8C%83%E5%BC%8F.PNG)

- 矩阵的Frobenius范数

![图片](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/d1cff533c866a51932decfa1c2111d5faca883af/picture/%E7%9F%A9%E9%98%B5%E8%8C%83%E5%BC%8F.PNG)

### 课后题

#### 1.证明一个矩阵A的转置的转置是A
核心：Aij=（AT）ji

所以：Aji=（Aij）T=（（AT）ji）T=Aji

#### 2.本节中定义了形状(2,3,4)的张量X。len(X)的输出结果是什么？
结果是：2
> len（张量）时，默认输出第一个维度
``` py
C = torch.arange(24).reshape(2, 3, 4)

C_0 = C.sum(axis=0)

C_1 = C.sum(axis=1)

C_2 = C.sum(axis=2)

C_0,C_1,C_2

C_0.shape,C_1.shape,C_2.shape          #结果：(torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3]))
```

#### 3.对于任意形状的张量X,len(X)是否总是对应于X特定轴的长度?这个轴是什么?
总是输出第一个轴的长度，对于一个二维张量，就是行数。

#### 4.运行A/A.sum(axis=1)，看看会发生什么。请分析一下原因？
``` py
A = torch.arange(20).reshape(5, 4)
A
#tensor([[ 0,  1,  2,  3],
#        [ 4,  5,  6,  7],
#        [ 8,  9, 10, 11],
#        [12, 13, 14, 15],
#        [16, 17, 18, 19]])
A=A.sum(axis=1)
A                  #tensor([ 6, 22, 38, 54, 70])
A.shape            #torch.Size([5])
print(len(A))      #5
A=A.sum(axis=0)
A                  #tensor([40, 45, 50, 55])
A.shape            #torch.Size([4])
print(len(A))      #4
```
经评论发现：
> 解决办法: A/A.sum(axis=1, keepdims=True), 在(5,4)和(5,1)之间是可以广播的

> 顺便一提, 如果是A/A.sum(axis=0), 则无需keepdims=True也可以正常广播
即在(5,4)和(4)间也是可以广播的，不需要一定是(5,4)和(1,4)

> **这可以理解为一个向量总是默认将其作为行向量，当尝试广播，对齐操作对象间的shape时，默认会做右对齐，因此(5,4)和(5)进行右对齐后发现维数不匹配，广播失败，而(5,4)和(4)之间可以广播**

> 抓住“右对齐”来理解广播机制是非常有好处的，判断任意tensor间是否可以广播，只需按照以下步骤就绝对不会出错了：
> 1. 将两操作对象的shape做右对齐
> 2. 空缺的位置假想为1
> 3. 比较同一位置处各操作对象的维数，若相同或有一个为1，则可以广播，否则无法广播

例如两个tensor的shape分别为(8, 1, 6, 5)和 (7, 1, 5)，那么是否可以广播呢？

做右对齐, 空缺的位置假想为1:

8, 1, 6, 5

1, 7, 1, 5

按照以上规则得出是可以广播的，操作结果的shape应为(8, 7, 6, 5)

#### 考虑一个具有形状(2,3,4)的张量，在轴0、1、2上的求和输出是什么形状?
``` py
C = torch.arange(24).reshape(2, 3, 4)

C_0 = C.sum(axis=0)

C_1 = C.sum(axis=1)

C_2 = C.sum(axis=2)

C_0,C_1,C_2

C_0.shape,C_1.shape,C_2.shape          #结果：(torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3]))
```

#### 为linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?
> 对于任意形状的张量，linalg.norm 函数默认计算的是该张量的 Frobenius 范数，即矩阵元素的平方和的平方根。
``` py
numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)

import numpy as np

# 计算向量的 2-范数
x = np.array([1, 2, 3])
norm_x = np.linalg.norm(x)
print(norm_x)  # 输出 3.7416573867739413

# 计算矩阵的 Frobenius 范数（矩阵元素的平方和的平方根）
A = np.array([[1, 2], [3, 4]])
norm_A = np.linalg.norm(A)
print(norm_A)  # 输出 5.477225575051661

# 计算矩阵每一行的 2-范数
norm_A_row = np.linalg.norm(A, axis=1)
print(norm_A_row)  # 输出 [2.23606798 5. ]
···

``` py
import numpy as np

# 创建一个矩阵
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# 计算矩阵的不同范数

# 1-范数（列和范数）
norm_1 = np.linalg.norm(A, ord=1)
print("1-范数:", norm_1)

# 2-范数（谱范数）
norm_2 = np.linalg.norm(A, ord=2)
print("2-范数:", norm_2)

# 无穷范数（行和范数）
norm_inf = np.linalg.norm(A, ord=np.inf)
print("无穷范数:", norm_inf)
```
