### 7.2VGG
1. 通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。VGG块内抽象了每个卷积层的结构，可以通过调用，方便的实现深度框架。
2. 其接口包含：块内卷积次数、输入输出通道数等参数
3. Sequential层包含四个维度[样本数,通道数,高度,宽度],经过Flatten后，变为[样本数，参数],所以参数=通道数*行*列
4. 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即3*3）比较浅层且宽的卷积更有效。正如从代码中所看到的，**我们在每个块的高度和宽度减半**，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。


### 7.3NiN

1. NiN使用由一个卷积层和多个1*1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
``` py
def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())
```
每个NiN块只有第一个卷积操作时自定义，其他全是单个像素上的卷积。

2. NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。
``` py
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    # 标签类别数是10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)),
    # 将四维的输出转成二维的输出，其形状为(批量大小,10)
    nn.Flatten())
```

NiN模型和AlexNet之间的一个显著区别是NiN完全取消了全连接层。 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。

``` py
#每层形状
Sequential output shape:     torch.Size([1, 96, 54, 54])
MaxPool2d output shape:      torch.Size([1, 96, 26, 26])
Sequential output shape:     torch.Size([1, 256, 26, 26])
MaxPool2d output shape:      torch.Size([1, 256, 12, 12])
Sequential output shape:     torch.Size([1, 384, 12, 12])
MaxPool2d output shape:      torch.Size([1, 384, 5, 5])
Dropout output shape:        torch.Size([1, 384, 5, 5])
Sequential output shape:     torch.Size([1, 10, 5, 5])
AdaptiveAvgPool2d output shape:      torch.Size([1, 10, 1, 1])
Flatten output shape:        torch.Size([1, 10])
```
3. 移除全连接层可减少过拟合，同时显著减少NiN的参数。

4. NiN的设计影响了许多后续卷积神经网络的设计

### 7.4 含并行连结的网络（GoogLeNet）
> 什么样大小的卷积核最合适的问题

> 优势：它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。

1. Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用1*1卷积层减少每像素级别上的通道维数从而降低模型复杂度。

2. GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。

3. GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。

### 7.5 批量规范化

拓展学习 [csdn关于批量规范化的理解](https://blog.csdn.net/jgj123321/article/details/105291672)

引入：
>训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。

>可持续加速深层网络的收敛速度
>
> 如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。

批量规范化层
1. 全连接层
仿射变换和激活函数之间。

2. 卷积层
卷积层之后和非线性激活函数之前；应用与每个batch中batch_size*h*w个像素;每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量,形状与每个输入x相同。

> 噪声带来的好处：优化中的各种噪声源通常会导致更快的训练和较少的过拟合

### 7.6 残差网络（ResNet）
引入：
> 随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。
>
> 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。

> 现代模型两大问题：
1. 梯度爆炸或梯度消失

2. 网络退化：层的堆叠，会使模型学习能力下降

小结
1. 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）（就是f(x)=x+g(x)中的x）较容易（尽管这是一个极端情况）。

2. 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。

3. 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
   
4. **即使f(x)=x+g(x)中的g(x)没有好的效果甚至倒退的效果，x也能保证f(x)保持原来的样子，不至于倒退，最多不变，这是解决网络退化的关键。**

### 7.7 稠密连接网络（DenseNet）
ResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[,]表示）而不是如ResNet的简单相加。 因此，在应用越来越复杂的函数序列后，我们执行从x到其展开式的映射：

$$
\mathbf{x} \to \left[
\mathbf{x},
f_1(\mathbf{x}),
f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right].
$$

核心：
> 每一个稠密块的结果都包含该块中每个卷积层后的结果，并在通道维度上叠加。

小结：
1. 在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。（很像Inception中的块，四合一，在通道上）
2. DenseNet的主要构建模块是稠密块和过渡层。
3. 在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。（1*1的卷积层）

