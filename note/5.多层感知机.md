> 4.1
> 1. [4.1课后题](#4_1_课后题)
>
> 4.2 mlp-scratch
> 1. [4.2课后题](#4_2课后题)
>
> 4.3 mlp-concise
> 1. [4.3课后题](#4_3课后题)
>
> 4.4 underfit-overfit
> 1. [4.4课后题](#4_4课后题)
### 为何要引入隐藏层
线性模型可能会出错，线性模型单调性但不是标准的线性相关。

    正相关：收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。

    违反单调性的例子：负相关我们可以使用与37摄氏度的距离作为特征。
 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。

### 多层感知机（MLP）
### 为什么要引入非线性的激活函数（activation function）？ 
 多个隐藏层的堆叠等价于对输入数据应用多次线性变换，其等价于一个线性变换；为提高模型的表达能力。
> 激活函数的输出被称为活性值（activations）

### 激活函数种类
1. **ReLU函数**        $\[ \text{ReLU}(x) = \max(x, 0) \]$;仅保留正元素并丢弃所有负元素。当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。
2. **参数化ReLU（Parameterized ReLU，pReLU） 函数** $\[ \text{pReLU}(x) = \max(0, x) + \alpha \min(0, x) \]$
3. **sigmoid函数**,(又称挤压函数（squashing function）)   $\[ \text{sigmoid}(x) = \frac{1}{1 + \exp(-x)} \]$  sigmoid函数将输入变换为区间(0, 1)上的输出。
> 注意其导数值的特称，两侧几乎为零，中间最大
4. **tanh函数** $\[ \text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} \]$  

### 






## 4_1_课后题

#### 1. 计算pReLU激活函数的导数。
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)
$$

 当x>0时，导数为1；其他为 $\alpha$



#### 证明 $\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$。

1. 证明方法一：

![代码证明](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/1adda89556c34c0e9c7585e8fd3e5e34608ce1b6/picture/4.1.1%E8%AF%BE%E5%90%8E%E9%A2%98%E7%AC%AC%E4%B8%89%E9%97%AE.png)

2. 证明方法二：
要证明 $\tanh(x) + 1 = 2 \sigma(2x)$，我们可以使用双曲正切函数 $\tanh(x)$ 和 Sigmoid 函数 $\sigma(x)$ 的定义以及一些性质来进行证明。首先，让我们回顾一下这两个函数的定义：

1. 双曲正切函数 $\tanh(x)$ 的定义为：

$$
\[
\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
$$

2. Sigmoid 函数 $\sigma(x)$ 的定义为：

$$
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
$$

现在我们开始证明：

$$
\[
\begin{aligned}
\tanh(x) + 1 &= \frac{e^x - e^{-x}}{e^x + e^{-x}} + 1 \\
&= \frac{e^x - e^{-x} + e^x + e^{-x}}{e^x + e^{-x}} \\
&= \frac{2e^x}{e^x + e^{-x}} \\
&= 2\frac{e^x}{e^x(1 + e^{-2x})} \\
&= 2\frac{1}{1 + e^{-2x}} \\
&= 2 \sigma(2x)
\end{aligned}
\]
$$

在上述推导中，我们使用了双曲正切函数和 Sigmoid 函数的定义，以及双曲正切函数的另一种表示形式 \(\tanh(x) = \frac{2}{1 + e^{-2x}}\)。通过这些步骤，我们得出了 $\tanh(x) + 1 = 2 \sigma(2x)$ 的结论，完成了证明。

### 3.证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。
> 根据评论区的大神讲解，获取一下信息
> 1. 含有非线性激活函数的多层感知机在有限空间内能逼近任意连续函数
> 2. 第二题的结论反过来也成立，任意一个连续的分段线性函数可以被使用ReLU（或pReLU）的多层感知机近似
> 
> **说明非线性激活函数的重要性，若没有，仅是线性变换不足以使模型完备**

### 4. 假设我们有一个非线性单元，将它一次应用于一个小批量的数据。这会导致什么样的问题？
> 根据评论区和GPT总结：
1. 批量归一化问题：如果对整个小批量数据进行批量归一化，即计算批量数据的均值和方差进行归一化操作，那么可能会导致归一化的效果不准确，因为每个样本可能具有不同的统计特征。这会影响批量归一化的稳定性和效果。
2. 梯度的不稳定性：非线性单元在反向传播时可能会导致梯度的不稳定性，特别是在深层网络中。这种不稳定性可能会导致梯度消失或梯度爆炸问题，使得模型的训练变得困难或者不稳定。
3. 计算效率：如果非线性单元的计算复杂度较高，一次应用于一个小批量的数据可能会导致计算量增加，影响模型的训练速度和效率。

## 4_2课后题

### 1. 在所有其他参数保持不变的情况下，更改超参数num_hiddens的值，并查看此超参数的变化对结果有何影响。确定此超参数的最佳值。
1. 128
   
   ![128](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/eb44f808dd3e924e3975b23aa7343e71ef6810e9/picture/4.2.1num_hiddens%3D128.png)
2. 256
   
   ![256](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2d436fcb96755b522b968520114a64391595a5cf/picture/4.2.1num_hiddens%3D256.png)
   
3. 512

   ![512](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/12efd6cd18119f8b521115e2931bf5c804f78759/picture/4.2.1num_hiddens%3D512.png)
   
### 2. 尝试添加更多的隐藏层，并查看它对结果有何影响。
> 结论放前面：**增加了隐藏层，收敛变慢了**

1. 一层

![一层hiddnes](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2d436fcb96755b522b968520114a64391595a5cf/picture/4.2.1num_hiddens%3D256.png)

2. 两层
``` py
#2024-4-20
#刘昊阳
num_inputs, num_outputs, num_hiddens,num_hiddens_2 = 784, 10, 128,64

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_hiddens_2, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_hiddens_2, requires_grad=True))
W3 = nn.Parameter(torch.randn(
    num_hiddens_2, num_outputs, requires_grad=True) * 0.01)
b3 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2,W3,b3]

def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    H2= relu(H@W2 + b2)
    return (H2@W3 + b3)
```
两层hiddens，256和64，epoch=10，lr=0.1

![两层hiddnes](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/9eae8ae78c12ae434f472d3304ac0ff8193e8180/picture/4.2.2%E4%B8%A4%E5%B1%82hidden64.png)

> **当隐藏层层数增加时，需要增大epoch，否则难收敛，效果不好；增加epoch后，效果会好**
> 见下图
3. 两层hiddens，256和64，epoch=20，lr=0.1

![两层且epoch=20](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/100795f5f20cc3a6ac19917d22f7d0bf77886422/picture/4.2.2%E4%B8%A4%E5%B1%82hidden64epoch%3D20.png)

### 3. 改变学习速率会如何影响结果？保持模型架构和其他超参数（包括轮数）不变，学习率设置为多少会带来最好的结果？
> 前几节做过这类实验，有个总结是：学习率太大，无法训练，学习率适当增大，能够使模型尽快收敛，学习率太小，收敛太慢。

examp：两层hiddens，256和64，epoch=20，lr=0.2

![两层hiddens，256和64，epoch=20，lr=0.2](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/dd249a7c0c14dd207ab58149f46501551e6d1249/picture/4.2.3%E4%B8%A4%E5%B1%82hiddens256%E5%92%8C64epoch%3D20lr%3D0.2.png)

> 两层hiddens，256和64，epoch=20，lr=0.2比两层hiddens，256和64，epoch=20，lr=0.1效果好
### 4. 通过对所有超参数（学习率、轮数、隐藏层数、每层的隐藏单元数）进行联合优化，可以得到的最佳结果是什么？
暂未得到最完美的

到目前为止，我做的最好的是两层hiddens，256和64，epoch=20，lr=0.2。
### 5. 描述为什么涉及多个超参数更具挑战性。
参数越多，需要优化的过程更长，增加计算量的同时，还有增加迭代次数，否则很难收敛。

### 6. 如果想要构建多个超参数的搜索方法，请想出一个聪明的策略。


## 4_3课后题
### 尝试添加不同数量的隐藏层（也可以修改学习率），怎么样设置效果最好？
参考4.2节课后题

### 尝试不同的激活函数，哪个效果最好？
> 评论区有人说：Sigmoid看上去更好，波动小，准确率高。
>
> 但经过我实验证明，他是最垃圾的，相比，tanh和relu都比她好，不分伯仲。如果非得选一个最好的，tanh。（评论区也有人和我相同观点）

1. RELU()

![RELU](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/ca1a35087efa6fdabeb6d502f5db8c2985a5c1a9/picture/4.3.2relu.png)

2. Sigmoid()

![sigmoid](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/ca1a35087efa6fdabeb6d502f5db8c2985a5c1a9/picture/4.3.2sigmoid.png)

3. tanh()

![tanh](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/ca1a35087efa6fdabeb6d502f5db8c2985a5c1a9/picture/4.3.2tanh.png)
### 尝试不同的方案来初始化权重，什么方法效果最好？


## 4_4课后题

### 1. 这个多项式回归问题可以准确地解出吗？提示：使用线性代数。
假设我们有如下的多项式回归问题：我们要拟合一个二次多项式$ \( y = w_0 + w_1x + w_2x^2 \)$。我们有一些样本数据 $\( (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \)$，我们的目标是找到最优的参数 $\( w_0, w_1, w_2 \)$ 来拟合这个二次多项式。

首先，我们可以构建特征矩阵 \( P \)，其中每一行代表一个样本，每一列代表特征的不同阶数。对于二次多项式，我们有：

$$
\[
P = \begin{bmatrix}
1 & x_1 & x_1^2 \\
1 & x_2 & x_2^2 \\
\vdots & \vdots & \vdots \\
1 & x_n & x_n^2 \\
\end{bmatrix}
\]
$$

同时，我们有标签向量 $\( y \)$，其中每个元素 $\( y_i \)$ 是对应样本的输出值。

接着，我们可以使用最小二乘法求解线性方程组 $\( P^T Pw = P^T y \)$。这个方程组可以写成：

$$
\[
\begin{bmatrix}
1 & 1 & \cdots & 1 \\
x_1 & x_2 & \cdots & x_n \\
x_1^2 & x_2^2 & \cdots & x_n^2 \\
\end{bmatrix}
\begin{bmatrix}
w_0 \\
w_1 \\
w_2 \\
\end{bmatrix}
$$

=

$$
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{bmatrix}
\]
$$

解这个线性方程组可以得到最优的参数 $\( w_0, w_1, w_2 \)$。然后我们就可以使用这些参数来拟合二次多项式，得到最终的拟合结果。

在实际应用中，我们可以利用现有的数值计算库，比如NumPy，来求解这个线性方程组，例如使用 `numpy.linalg.solve` 函数。以下是一个简单的Python示例代码：

```python
import numpy as np

# 样本数据
X = np.array([1, 2, 3, 4, 5])  # 输入特征
y = np.array([2.1, 3.9, 7.2, 10.8, 17.1])  # 输出标签

# 构建特征矩阵
P = np.vstack([np.ones_like(X), X, X**2]).T

# 使用最小二乘法求解线性方程组
w = np.linalg.solve(P.T @ P, P.T @ y)

# 输出最优参数
print("最优参数 w0:", w[0])
print("最优参数 w1:", w[1])
print("最优参数 w2:", w[2])
```

这段代码会输出拟合的二次多项式的最优参数 $\( w_0, w_1, w_2 \)$。然后我们可以使用这些参数来进行预测或者绘制拟合曲线。

### 考虑多项式的模型选择。

1. 绘制训练损失与模型复杂度（多项式的阶数）的关系图。观察到了什么？需要多少阶的多项式才能将训练损失减少到0?
``` py
# 随着feature_num的增加 loss的变化
def train1(train_features,test_features,train_labels,
          test_labels,num_epochs= 400):
    loss = nn.MSELoss(reduction='none')
    input_shape = train_features.shape[-1]
    
    net = nn.Sequential(nn.Linear(input_shape,1,bias=False))
    batch_size = min(10,train_labels.shape[0])
    train_iter = d2l.load_array((train_features,
                                 train_labels.reshape(-1,1)),batch_size)
    test_iter = d2l.load_array((test_features,
                               test_labels.reshape(-1,1)),batch_size,is_train=False)

    trainer = torch.optim.SGD(net.parameters(),lr=0.01)
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 399:
            train_loss=evaluate_loss(net,train_iter,loss)
            test_loss = evaluate_loss(net,test_iter,loss)
            return train_loss,test_loss
            
import matplotlib.pyplot as plt

a = range(1,20)

animator = d2l.Animator(xlabel='feature_nums', ylabel='loss', yscale='log',
                            xlim=[1, 20], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
for i in a:
    c=0
    d=0
    c,d=train1(poly_features[:n_train,:i],poly_features[n_train:,:i],
         labels[:n_train],labels[n_train:])
    animator.add(i,(c,d))        
```


2. 在这种情况下绘制测试的损失图。

> 借用评论区的结论：随着 feature_nums 的增加模型的 loss 趋于稳定，虽然没有最低点nums=4效果好，​ 但是 误差已经很小了，测试集和训练集相差结果也不是很大，过拟合可以接受
>
> 本人不会画图，借鉴了一下
> 
![feature_num=4时结果最好]()

3. 生成同样的图，作为数据量的函数。
> 结论：  一开始明显过拟合 ，数据量太小 ，训练集误差小，测试集误差大；随着训练数据量的增加，训练集和测试集的loss 差距逐渐减小；80个epochs 后二者都趋于平稳
>
> 可能会发生上溢出 ， 超过数据表示范围 ，而且数据太大也不利于计算
>
> 误差误差，泛化误差和训练误差都不可能为0(除非只有一个样本，训练误差为0)

``` py
def train2(train_features,test_features,train_labels,
          test_labels,num_epochs=400):
    loss = nn.MSELoss(reduction='none')
    input_shape = train_features.shape[-1]
    
    net = nn.Sequential(nn.Linear(input_shape,1,bias=False))
    batch_size = min(10,train_labels.shape[0])
    train_iter = d2l.load_array((train_features,
                                 train_labels.reshape(-1,1)),batch_size)
    test_iter = d2l.load_array((test_features,
                               test_labels.reshape(-1,1)),batch_size,is_train=False)

    trainer = torch.optim.SGD(net.parameters(),lr=0.01)
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == (num_epochs-1):
            return evaluate_loss(net, train_iter, loss),evaluate_loss(net, test_iter, loss)




b= range(1,n_train)

anm = d2l.Animator(xlabel='data_nums',ylabel='loss',yscale='log',
                   xlim=[1,n_train],ylim=[1e-3, 1e2],
                  legend=['train','test'])
for i in b:
    c=0
    d=0
    c,d = train2(poly_features[:i,:4],poly_features[n_train:,:4],
                 labels[:i],labels[n_train:])
    anm.add(i,(c,d))
```
![loss]()

