> 4.1
> 1. [4.1课后题](#4_1_课后题)
>
> 4.2 mlp-scratch
> 1. [4.2课后题](#4_2课后题)
>
> 4.3 mlp-concise
> 1. [4.3课后题](#4_3课后题)
### 为何要引入隐藏层
线性模型可能会出错，线性模型单调性但不是标准的线性相关。

    正相关：收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。

    违反单调性的例子：负相关我们可以使用与37摄氏度的距离作为特征。
 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。

### 多层感知机（MLP）
### 为什么要引入非线性的激活函数（activation function）？ 
 多个隐藏层的堆叠等价于对输入数据应用多次线性变换，其等价于一个线性变换；为提高模型的表达能力。
> 激活函数的输出被称为活性值（activations）

### 激活函数种类
1. **ReLU函数**        $\[ \text{ReLU}(x) = \max(x, 0) \]$;仅保留正元素并丢弃所有负元素。当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。
2. **参数化ReLU（Parameterized ReLU，pReLU） 函数** $\[ \text{pReLU}(x) = \max(0, x) + \alpha \min(0, x) \]$
3. **sigmoid函数**,(又称挤压函数（squashing function）)   $\[ \text{sigmoid}(x) = \frac{1}{1 + \exp(-x)} \]$  sigmoid函数将输入变换为区间(0, 1)上的输出。
> 注意其导数值的特称，两侧几乎为零，中间最大
4. **tanh函数** $\[ \text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} \]$  

### 






## 4_1_课后题

#### 1. 计算pReLU激活函数的导数。
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)
$$

 当x>0时，导数为1；其他为 $\alpha$



#### 证明 $\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$。

1. 证明方法一：

![代码证明](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/1adda89556c34c0e9c7585e8fd3e5e34608ce1b6/picture/4.1.1%E8%AF%BE%E5%90%8E%E9%A2%98%E7%AC%AC%E4%B8%89%E9%97%AE.png)

2. 证明方法二：
要证明 $\tanh(x) + 1 = 2 \sigma(2x)$，我们可以使用双曲正切函数 $\tanh(x)$ 和 Sigmoid 函数 $\sigma(x)$ 的定义以及一些性质来进行证明。首先，让我们回顾一下这两个函数的定义：

1. 双曲正切函数 $\tanh(x)$ 的定义为：

$$
\[
\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
$$

2. Sigmoid 函数 $\sigma(x)$ 的定义为：

$$
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
$$

现在我们开始证明：

$$
\[
\begin{aligned}
\tanh(x) + 1 &= \frac{e^x - e^{-x}}{e^x + e^{-x}} + 1 \\
&= \frac{e^x - e^{-x} + e^x + e^{-x}}{e^x + e^{-x}} \\
&= \frac{2e^x}{e^x + e^{-x}} \\
&= 2\frac{e^x}{e^x(1 + e^{-2x})} \\
&= 2\frac{1}{1 + e^{-2x}} \\
&= 2 \sigma(2x)
\end{aligned}
\]
$$

在上述推导中，我们使用了双曲正切函数和 Sigmoid 函数的定义，以及双曲正切函数的另一种表示形式 \(\tanh(x) = \frac{2}{1 + e^{-2x}}\)。通过这些步骤，我们得出了 $\tanh(x) + 1 = 2 \sigma(2x)$ 的结论，完成了证明。

### 3.证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。
> 根据评论区的大神讲解，获取一下信息
> 1. 含有非线性激活函数的多层感知机在有限空间内能逼近任意连续函数
> 2. 第二题的结论反过来也成立，任意一个连续的分段线性函数可以被使用ReLU（或pReLU）的多层感知机近似
> 
> **说明非线性激活函数的重要性，若没有，仅是线性变换不足以使模型完备**

### 4. 假设我们有一个非线性单元，将它一次应用于一个小批量的数据。这会导致什么样的问题？
> 根据评论区和GPT总结：
1. 批量归一化问题：如果对整个小批量数据进行批量归一化，即计算批量数据的均值和方差进行归一化操作，那么可能会导致归一化的效果不准确，因为每个样本可能具有不同的统计特征。这会影响批量归一化的稳定性和效果。
2. 梯度的不稳定性：非线性单元在反向传播时可能会导致梯度的不稳定性，特别是在深层网络中。这种不稳定性可能会导致梯度消失或梯度爆炸问题，使得模型的训练变得困难或者不稳定。
3. 计算效率：如果非线性单元的计算复杂度较高，一次应用于一个小批量的数据可能会导致计算量增加，影响模型的训练速度和效率。

## 4_2课后题

### 1. 在所有其他参数保持不变的情况下，更改超参数num_hiddens的值，并查看此超参数的变化对结果有何影响。确定此超参数的最佳值。
1. 128
   
   ![128](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/eb44f808dd3e924e3975b23aa7343e71ef6810e9/picture/4.2.1num_hiddens%3D128.png)
2. 256
   
   ![256](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2d436fcb96755b522b968520114a64391595a5cf/picture/4.2.1num_hiddens%3D256.png)
   
3. 512

   ![512](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/12efd6cd18119f8b521115e2931bf5c804f78759/picture/4.2.1num_hiddens%3D512.png)
   
### 2. 尝试添加更多的隐藏层，并查看它对结果有何影响。
> 结论放前面：**增加了隐藏层，收敛变慢了**

1. 一层

![一层hiddnes](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2d436fcb96755b522b968520114a64391595a5cf/picture/4.2.1num_hiddens%3D256.png)

2. 两层
``` py
#2024-4-20
#刘昊阳
num_inputs, num_outputs, num_hiddens,num_hiddens_2 = 784, 10, 128,64

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_hiddens_2, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_hiddens_2, requires_grad=True))
W3 = nn.Parameter(torch.randn(
    num_hiddens_2, num_outputs, requires_grad=True) * 0.01)
b3 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2,W3,b3]

def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    H2= relu(H@W2 + b2)
    return (H2@W3 + b3)
```
两层hiddens，256和64，epoch=10，lr=0.1

![两层hiddnes](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/9eae8ae78c12ae434f472d3304ac0ff8193e8180/picture/4.2.2%E4%B8%A4%E5%B1%82hidden64.png)

> **当隐藏层层数增加时，需要增大epoch，否则难收敛，效果不好；增加epoch后，效果会好**
> 见下图
3. 两层hiddens，256和64，epoch=20，lr=0.1

![两层且epoch=20](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/100795f5f20cc3a6ac19917d22f7d0bf77886422/picture/4.2.2%E4%B8%A4%E5%B1%82hidden64epoch%3D20.png)

### 3. 改变学习速率会如何影响结果？保持模型架构和其他超参数（包括轮数）不变，学习率设置为多少会带来最好的结果？
> 前几节做过这类实验，有个总结是：学习率太大，无法训练，学习率适当增大，能够使模型尽快收敛，学习率太小，收敛太慢。

examp：两层hiddens，256和64，epoch=20，lr=0.2

![两层hiddens，256和64，epoch=20，lr=0.2](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/dd249a7c0c14dd207ab58149f46501551e6d1249/picture/4.2.3%E4%B8%A4%E5%B1%82hiddens256%E5%92%8C64epoch%3D20lr%3D0.2.png)

> 两层hiddens，256和64，epoch=20，lr=0.2比两层hiddens，256和64，epoch=20，lr=0.1效果好
### 4. 通过对所有超参数（学习率、轮数、隐藏层数、每层的隐藏单元数）进行联合优化，可以得到的最佳结果是什么？
暂未得到最完美的

到目前为止，我做的最好的是两层hiddens，256和64，epoch=20，lr=0.2。
### 5. 描述为什么涉及多个超参数更具挑战性。
参数越多，需要优化的过程更长，增加计算量的同时，还有增加迭代次数，否则很难收敛。

### 6. 如果想要构建多个超参数的搜索方法，请想出一个聪明的策略。


## 4_3课后题
### 尝试添加不同数量的隐藏层（也可以修改学习率），怎么样设置效果最好？
参考4.2节课后题

### 尝试不同的激活函数，哪个效果最好？
> 评论区有人说：Sigmoid看上去更好，波动小，准确率高。
>
> 但经过我实验证明，他是最垃圾的，相比，tanh和relu都比她好，不分伯仲。如果非得选一个最好的，tanh。（评论区也有人和我相同观点）

1. RELU()

![RELU](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/ca1a35087efa6fdabeb6d502f5db8c2985a5c1a9/picture/4.3.2relu.png)

2. Sigmoid()

![sigmoid](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/ca1a35087efa6fdabeb6d502f5db8c2985a5c1a9/picture/4.3.2sigmoid.png)

3. tanh()

![tanh](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/ca1a35087efa6fdabeb6d502f5db8c2985a5c1a9/picture/4.3.2tanh.png)
### 尝试不同的方案来初始化权重，什么方法效果最好？
