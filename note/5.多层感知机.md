
### 为何要引入隐藏层
线性模型可能会出错，线性模型单调性但不是标准的线性相关。

    正相关：收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。

    违反单调性的例子：负相关我们可以使用与37摄氏度的距离作为特征。
 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。

### 多层感知机（MLP）
### 为什么要引入非线性的激活函数（activation function）？ 
 多个隐藏层的堆叠等价于对输入数据应用多次线性变换，其等价于一个线性变换；为提高模型的表达能力。
> 激活函数的输出被称为活性值（activations）

### 激活函数种类
1. **ReLU函数**        $\[ \text{ReLU}(x) = \max(x, 0) \]$;仅保留正元素并丢弃所有负元素。当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。
2. **参数化ReLU（Parameterized ReLU，pReLU） 函数** $\[ \text{pReLU}(x) = \max(0, x) + \alpha \min(0, x) \]$
3. **sigmoid函数**,(又称挤压函数（squashing function）)   $\[ \text{sigmoid}(x) = \frac{1}{1 + \exp(-x)} \]$  sigmoid函数将输入变换为区间(0, 1)上的输出。
> 注意其导数值的特称，两侧几乎为零，中间最大
4. **tanh函数** $\[ \text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} \]$  














> 4.1
> 1. [4.1课后题](#4_1_课后题)

## 4_1_课后题

#### 1. 计算pReLU激活函数的导数。
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)
$$

 当x>0时，导数为1；其他为 $\alpha$

#### 2. 证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。

#### 证明 $\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$。

要证明 $\tanh(x) + 1 = 2 \sigma(2x)$，我们可以使用双曲正切函数 $\tanh(x)$ 和 Sigmoid 函数 $\sigma(x)$ 的定义以及一些性质来进行证明。首先，让我们回顾一下这两个函数的定义：

1. 双曲正切函数 $\tanh(x)$ 的定义为：

$$
\[
\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
$$

2. Sigmoid 函数 $\sigma(x)$ 的定义为：

$$
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
$$

现在我们开始证明：

$$
\[
\begin{aligned}
\tanh(x) + 1 &= \frac{e^x - e^{-x}}{e^x + e^{-x}} + 1 \\
&= \frac{e^x - e^{-x} + e^x + e^{-x}}{e^x + e^{-x}} \\
&= \frac{2e^x}{e^x + e^{-x}} \\
&= 2\frac{e^x}{e^x(1 + e^{-2x})} \\
&= 2\frac{1}{1 + e^{-2x}} \\
&= 2 \sigma(2x)
\end{aligned}
\]
$$

在上述推导中，我们使用了双曲正切函数和 Sigmoid 函数的定义，以及双曲正切函数的另一种表示形式 \(\tanh(x) = \frac{2}{1 + e^{-2x}}\)。通过这些步骤，我们得出了 $\tanh(x) + 1 = 2 \sigma(2x)$ 的结论，完成了证明。
