> 4.1
> 1. [4.1课后题](#4_1_课后题)
>
> 4.2 mlp-scratch
> 1. [4.2课后题](#4_2课后题)
### 为何要引入隐藏层
线性模型可能会出错，线性模型单调性但不是标准的线性相关。

    正相关：收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。

    违反单调性的例子：负相关我们可以使用与37摄氏度的距离作为特征。
 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。

### 多层感知机（MLP）
### 为什么要引入非线性的激活函数（activation function）？ 
 多个隐藏层的堆叠等价于对输入数据应用多次线性变换，其等价于一个线性变换；为提高模型的表达能力。
> 激活函数的输出被称为活性值（activations）

### 激活函数种类
1. **ReLU函数**        $\[ \text{ReLU}(x) = \max(x, 0) \]$;仅保留正元素并丢弃所有负元素。当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。
2. **参数化ReLU（Parameterized ReLU，pReLU） 函数** $\[ \text{pReLU}(x) = \max(0, x) + \alpha \min(0, x) \]$
3. **sigmoid函数**,(又称挤压函数（squashing function）)   $\[ \text{sigmoid}(x) = \frac{1}{1 + \exp(-x)} \]$  sigmoid函数将输入变换为区间(0, 1)上的输出。
> 注意其导数值的特称，两侧几乎为零，中间最大
4. **tanh函数** $\[ \text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} \]$  

### 






## 4_1_课后题

#### 1. 计算pReLU激活函数的导数。
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)
$$

 当x>0时，导数为1；其他为 $\alpha$



#### 证明 $\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$。

1. 证明方法一：

![代码证明](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/1adda89556c34c0e9c7585e8fd3e5e34608ce1b6/picture/4.1.1%E8%AF%BE%E5%90%8E%E9%A2%98%E7%AC%AC%E4%B8%89%E9%97%AE.png)

2. 证明方法二：
要证明 $\tanh(x) + 1 = 2 \sigma(2x)$，我们可以使用双曲正切函数 $\tanh(x)$ 和 Sigmoid 函数 $\sigma(x)$ 的定义以及一些性质来进行证明。首先，让我们回顾一下这两个函数的定义：

1. 双曲正切函数 $\tanh(x)$ 的定义为：

$$
\[
\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
$$

2. Sigmoid 函数 $\sigma(x)$ 的定义为：

$$
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
$$

现在我们开始证明：

$$
\[
\begin{aligned}
\tanh(x) + 1 &= \frac{e^x - e^{-x}}{e^x + e^{-x}} + 1 \\
&= \frac{e^x - e^{-x} + e^x + e^{-x}}{e^x + e^{-x}} \\
&= \frac{2e^x}{e^x + e^{-x}} \\
&= 2\frac{e^x}{e^x(1 + e^{-2x})} \\
&= 2\frac{1}{1 + e^{-2x}} \\
&= 2 \sigma(2x)
\end{aligned}
\]
$$

在上述推导中，我们使用了双曲正切函数和 Sigmoid 函数的定义，以及双曲正切函数的另一种表示形式 \(\tanh(x) = \frac{2}{1 + e^{-2x}}\)。通过这些步骤，我们得出了 $\tanh(x) + 1 = 2 \sigma(2x)$ 的结论，完成了证明。

### 3.证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。
> 根据评论区的大神讲解，获取一下信息
> 1. 含有非线性激活函数的多层感知机在有限空间内能逼近任意连续函数
> 2. 第二题的结论反过来也成立，任意一个连续的分段线性函数可以被使用ReLU（或pReLU）的多层感知机近似
> 
> **说明非线性激活函数的重要性，若没有，仅是线性变换不足以使模型完备**

### 4. 假设我们有一个非线性单元，将它一次应用于一个小批量的数据。这会导致什么样的问题？
> 根据评论区和GPT总结：
1. 批量归一化问题：如果对整个小批量数据进行批量归一化，即计算批量数据的均值和方差进行归一化操作，那么可能会导致归一化的效果不准确，因为每个样本可能具有不同的统计特征。这会影响批量归一化的稳定性和效果。
2. 梯度的不稳定性：非线性单元在反向传播时可能会导致梯度的不稳定性，特别是在深层网络中。这种不稳定性可能会导致梯度消失或梯度爆炸问题，使得模型的训练变得困难或者不稳定。
3. 计算效率：如果非线性单元的计算复杂度较高，一次应用于一个小批量的数据可能会导致计算量增加，影响模型的训练速度和效率。

## 4_2课后题

### 1. 在所有其他参数保持不变的情况下，更改超参数num_hiddens的值，并查看此超参数的变化对结果有何影响。确定此超参数的最佳值。
1. 128
   
   ![128](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/eb44f808dd3e924e3975b23aa7343e71ef6810e9/picture/4.2.1num_hiddens%3D128.png)
2. 256
   
   ![256](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2d436fcb96755b522b968520114a64391595a5cf/picture/4.2.1num_hiddens%3D256.png)
   
3. 512

   ![512](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/12efd6cd18119f8b521115e2931bf5c804f78759/picture/4.2.1num_hiddens%3D512.png)
   
### 2. 尝试添加更多的隐藏层，并查看它对结果有何影响。
> 结论放前面：**增加了隐藏层，收敛变慢了**

1. 一层

![一层hiddnes](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/2d436fcb96755b522b968520114a64391595a5cf/picture/4.2.1num_hiddens%3D256.png)
2. 两层
``` py
#2024-4-20
#刘昊阳
num_inputs, num_outputs, num_hiddens,num_hiddens_2 = 784, 10, 128,64

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_hiddens_2, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_hiddens_2, requires_grad=True))
W3 = nn.Parameter(torch.randn(
    num_hiddens_2, num_outputs, requires_grad=True) * 0.01)
b3 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2,W3,b3]

def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    H2= relu(H@W2 + b2)
    return (H2@W3 + b3)
```

![两层hiddnes](https://github.com/Sheibyer/Introduction-to-machine-learning/blob/9eae8ae78c12ae434f472d3304ac0ff8193e8180/picture/4.2.2%E4%B8%A4%E5%B1%82hidden64.png)

3. 两层且epoch=20

![两层且epoch=20]()
